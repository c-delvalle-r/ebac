{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Módulo 47 - Big Data 1\n",
    "\n",
    "## Spark\n",
    "Spark es una herramienta de apache para la gestión y manejo de Big Data. Es un framework de procesamiento de información que puede generar acciones rápidas sobre cojuntos de datos muy grandes. Tiene capacidades de procesamiento distribuido en varios servidores; ideal para tratamiento de datos de machine learning.\n",
    "\n",
    "La arquitectura esta diseñada para escalar plataformas. Tiene un **Driver** que es el que convierte en código del usuario en tareas, que son a su vez ejecutadas por los **Executors** que habitan en los *Work Nodes* y que pueden estar distribuidos en los distintos servers. Generalmente existe una capa de manejo de Clusters para que  los Workers se auto adapten a la demanda del sistema. Esta capa puede ser Hadoop Yarn, Kubernetes, Docker Swarm, etc.\n",
    "\n",
    "### ¿Por qué Spark?\n",
    "1. Velocidad - Tiene un data-engine en memoria que puede reducir mucho su tiempo de ejecución de hasta 10x en tareas complejas respecto a su tecnología más parecida\n",
    "2. Spark API Amigable - La facilidad de llamar a la API de Spark lo hace muy atractivo. La implementación de la API esconde mucho del a complejidad de configurar y ejecutar un sistema tan complejo\n",
    "\n",
    "### Conceptos importantes Spark\n",
    "1. RDD (Resilient Distributed Dataset) Una abstracción para una colección de objetos que puede estar distribbuida en un cluster de servers. Pueden ser archivos de texto, bases de datos SQL, buckets S3, etc. El API de Spark es´ta construido sobre este concepto, donde se pueden hacer operaciones de unir data sets, filtrar, samplear datos, agregarlos, etc.\n",
    "2. Spark SQL - Originalmente llamdo Shark. Es la interfaz más conocida para tratar datos en Spark. Se usa el concepto de dataframe, implementación de SQL compliant con todos los estándares.\n",
    "\n",
    "### Otras librerias de Spark\n",
    "- Mllib - Framework para pipelines de Machine Learning - K-means, random forests, clasificación, regresión, etc.\n",
    "- GraphX - Procesamiento de estructura de imágenes Big Data. Se traducen los gráficos a dataframes, además de usar Catalyst\n",
    "- Streaming - Procesamiento de información en tiempo real. Se divide la información en \"micro-batches\" orquestados por la API de spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.5.1\n"
     ]
    }
   ],
   "source": [
    "# Librerias a importar\n",
    "import sys\n",
    "from random import random\n",
    "from operator import add\n",
    "import os\n",
    "\n",
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "import pyspark \n",
    "print(pyspark.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession\\\n",
    "            .builder\\\n",
    "            .master(\"local\")\\\n",
    "            .appName(\"PythonPi\")\\\n",
    "            .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Out of stack overflow forum\n",
    "os.environ['PYSPARK_DRIVER_PYTHON_OPTS']= \"notebook\"\n",
    "os.environ['PYSPARK_DRIVER_PYTHON'] = sys.executable\n",
    "os.environ['PYSPARK_PYTHON'] = sys.executable\n",
    "\n",
    "import os\n",
    "import sys\n",
    "spark_path = \"C:\\Spark\\spark-3.5.1-bin-hadoop3\" # spark installed folder\n",
    "os.environ['SPARK_HOME'] = spark_path\n",
    "sys.path.insert(0, spark_path + \"/bin\")\n",
    "sys.path.insert(0, spark_path + \"/python/pyspark/\")\n",
    "sys.path.insert(0, spark_path + \"/python/lib/pyspark.zip\")\n",
    "sys.path.insert(0, spark_path + \"/python/lib/py4j-0.10.9.7-src.zip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pi is roughly 3.140040\n"
     ]
    }
   ],
   "source": [
    "#Ejemplo Simple \n",
    "partitions = 4\n",
    "n = 100000 * partitions\n",
    "\n",
    "def f(_):\n",
    "    x = random()*2 - 1\n",
    "    y = random()*2 - 1\n",
    "    return 1 if x**2 + y**2 <= 1 else 0\n",
    "\n",
    "count = spark.sparkContext.parallelize(range(1, n+1), partitions).map(f).reduce(add)\n",
    "print('Pi is roughly %f' % (4.0 * count / n))\n",
    "\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Operaciones básicas de Spark\n",
    "- Lectura de Archivos\n",
    "- Análisis inicial de datos\n",
    "    - Crear sesión de Spark\n",
    "    - Generar un Schema y llenarlo de datos de un CSV\n",
    "    - Contar el número de filas, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importar sesión de Spark\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Generar sesión\n",
    "spark = SparkSession\\\n",
    "            .builder\\\n",
    "            .appName(\"LecturaArchivoCSV\")\\\n",
    "            .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Leer archivo de videojuegos\n",
    "path_archivo = \"D:/DataAnalysis_EBAC/ebac/Python/Modulo19/vgsales.csv\"\n",
    "df = spark.read.csv(path_archivo, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.dataframe.DataFrame"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Nota como es un dataframe de Spark, no pandas\n",
    "type(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------------------+--------+----+------------+---------+--------+--------+--------+-----------+------------+\n",
      "|Rank|                Name|Platform|Year|       Genre|Publisher|NA_Sales|EU_Sales|JP_Sales|Other_Sales|Global_Sales|\n",
      "+----+--------------------+--------+----+------------+---------+--------+--------+--------+-----------+------------+\n",
      "|   1|          Wii Sports|     Wii|2006|      Sports| Nintendo|   41.49|   29.02|    3.77|       8.46|       82.74|\n",
      "|   2|   Super Mario Bros.|     NES|1985|    Platform| Nintendo|   29.08|    3.58|    6.81|       0.77|       40.24|\n",
      "|   3|      Mario Kart Wii|     Wii|2008|      Racing| Nintendo|   15.85|   12.88|    3.79|       3.31|       35.82|\n",
      "|   4|   Wii Sports Resort|     Wii|2009|      Sports| Nintendo|   15.75|   11.01|    3.28|       2.96|          33|\n",
      "|   5|Pokemon Red/Pokem...|      GB|1996|Role-Playing| Nintendo|   11.27|    8.89|   10.22|          1|       31.37|\n",
      "|   6|              Tetris|      GB|1989|      Puzzle| Nintendo|    23.2|    2.26|    4.22|       0.58|       30.26|\n",
      "|   7|New Super Mario B...|      DS|2006|    Platform| Nintendo|   11.38|    9.23|     6.5|        2.9|       30.01|\n",
      "|   8|            Wii Play|     Wii|2006|        Misc| Nintendo|   14.03|     9.2|    2.93|       2.85|       29.02|\n",
      "|   9|New Super Mario B...|     Wii|2009|    Platform| Nintendo|   14.59|    7.06|     4.7|       2.26|       28.62|\n",
      "|  10|           Duck Hunt|     NES|1984|     Shooter| Nintendo|   26.93|    0.63|    0.28|       0.47|       28.31|\n",
      "+----+--------------------+--------+----+------------+---------+--------+--------+--------+-----------+------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Rank', 'string'),\n",
       " ('Name', 'string'),\n",
       " ('Platform', 'string'),\n",
       " ('Year', 'string'),\n",
       " ('Genre', 'string'),\n",
       " ('Publisher', 'string'),\n",
       " ('NA_Sales', 'string'),\n",
       " ('EU_Sales', 'string'),\n",
       " ('JP_Sales', 'string'),\n",
       " ('Other_Sales', 'string'),\n",
       " ('Global_Sales', 'string')]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Typos de datos\n",
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Rank', 'string'),\n",
       " ('Name', 'string'),\n",
       " ('Platform', 'string'),\n",
       " ('Year', 'string'),\n",
       " ('Genre', 'string'),\n",
       " ('Publisher', 'string'),\n",
       " ('NA_Sales', 'float'),\n",
       " ('EU_Sales', 'float'),\n",
       " ('JP_Sales', 'float'),\n",
       " ('Other_Sales', 'float'),\n",
       " ('Global_Sales', 'float')]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cambiar números de string a float\n",
    "from pyspark.sql.types import IntegerType, FloatType\n",
    "\n",
    "df = df.withColumn('NA_Sales', df.NA_Sales.cast(FloatType()))\n",
    "df = df.withColumn('EU_Sales', df.EU_Sales.cast(FloatType()))\n",
    "df = df.withColumn('JP_Sales', df.JP_Sales.cast(FloatType()))\n",
    "df = df.withColumn('Other_Sales', df.Other_Sales.cast(FloatType()))\n",
    "df = df.withColumn('Global_Sales', df.Global_Sales.cast(FloatType()))\n",
    "\n",
    "df.dtypes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Rank: string (nullable = true)\n",
      " |-- Name: string (nullable = true)\n",
      " |-- Platform: string (nullable = true)\n",
      " |-- Year: string (nullable = true)\n",
      " |-- Genre: string (nullable = true)\n",
      " |-- Publisher: string (nullable = true)\n",
      " |-- NA_Sales: float (nullable = true)\n",
      " |-- EU_Sales: float (nullable = true)\n",
      " |-- JP_Sales: float (nullable = true)\n",
      " |-- Other_Sales: float (nullable = true)\n",
      " |-- Global_Sales: float (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Imprime la información de columnas de una manera jerárquica\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---------------+--------+----+--------+------------+--------+--------+--------+-----------+------------+\n",
      "|Rank|           Name|Platform|Year|   Genre|   Publisher|NA_Sales|EU_Sales|JP_Sales|Other_Sales|Global_Sales|\n",
      "+----+---------------+--------+----+--------+------------+--------+--------+--------+-----------+------------+\n",
      "| 545|Missile Command|    2600|1980| Shooter|       Atari|    2.56|    0.17|     0.0|       0.03|        2.76|\n",
      "|1431|      Centipede|    2600|1981| Shooter|       Atari|    1.26|    0.08|     0.0|       0.01|        1.36|\n",
      "| 608| Space Invaders|    2600| N/A| Shooter|       Atari|    2.36|    0.14|     0.0|       0.03|        2.53|\n",
      "| 240|       Pitfall!|    2600|1981|Platform|  Activision|    4.21|    0.24|     0.0|       0.05|         4.5|\n",
      "| 736|        Frogger|    2600|1981|  Action|Parker Bros.|    2.06|    0.12|     0.0|       0.02|         2.2|\n",
      "|1108|    Ms. Pac-Man|    2600|1981|  Puzzle|       Atari|    1.54|     0.1|     0.0|       0.02|        1.65|\n",
      "|1117|        Dig Dug|    2600|1982|  Puzzle|       Atari|    1.52|     0.1|     0.0|       0.02|        1.64|\n",
      "|  90|        Pac-Man|    2600|1982|  Puzzle|       Atari|    7.28|    0.45|     0.0|       0.08|        7.81|\n",
      "|1155|     River Raid|    2600|1981| Shooter|  Activision|    1.49|    0.09|     0.0|       0.02|         1.6|\n",
      "| 768|   Demon Attack|    2600|1981| Shooter|      Imagic|    1.99|    0.12|     0.0|       0.02|        2.13|\n",
      "+----+---------------+--------+----+--------+------------+--------+--------+--------+-----------+------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Ordenamiento de Resuldados con .sort()\n",
    "import pyspark.sql.functions as F1\n",
    "\n",
    "# Se ordena por una columna\n",
    "df.sort(F1.col(\"Platform\")).show(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16598"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------------------+--------+----+---------+-----------+--------+--------+--------+-----------+------------+\n",
      "|Rank|                Name|Platform|Year|    Genre|  Publisher|NA_Sales|EU_Sales|JP_Sales|Other_Sales|Global_Sales|\n",
      "+----+--------------------+--------+----+---------+-----------+--------+--------+--------+-----------+------------+\n",
      "|4380|Maze Craze: A Gam...|    2600| N/A|   Action|      Atari|    0.42|    0.02|     0.0|        0.0|        0.45|\n",
      "|4471|      Super Breakout|    2600| N/A|   Puzzle|      Atari|    0.41|    0.03|     0.0|        0.0|        0.44|\n",
      "|5063|             Hangman|    2600| N/A|   Puzzle|      Atari|    0.35|    0.02|     0.0|        0.0|        0.38|\n",
      "|1515|           Adventure|    2600| N/A|Adventure|      Atari|    1.21|    0.08|     0.0|       0.01|         1.3|\n",
      "|5659|            Dragster|    2600| N/A|   Racing| Activision|     0.3|    0.02|     0.0|        0.0|        0.32|\n",
      "|2115|      Air-Sea Battle|    2600| N/A|  Shooter|      Atari|    0.91|    0.06|     0.0|       0.01|        0.98|\n",
      "|5800|        Slot Machine|    2600| N/A|   Action|      Atari|    0.29|    0.02|     0.0|        0.0|        0.31|\n",
      "|4153|              Karate|    2600| N/A| Fighting|Ultravision|    0.44|    0.03|     0.0|        0.0|        0.47|\n",
      "|6285|            Indy 500|    2600| N/A|   Racing|      Atari|    0.26|    0.01|     0.0|        0.0|        0.27|\n",
      "| 608|      Space Invaders|    2600| N/A|  Shooter|      Atari|    2.36|    0.14|     0.0|       0.03|        2.53|\n",
      "+----+--------------------+--------+----+---------+-----------+--------+--------+--------+-----------+------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Ordenar por dos columnas\n",
    "df.sort(F1.col(\"Platform\"), F1.col('Year').desc()).show(10)\n",
    "\n",
    "# Nota como no se usan corchetes, solamente se listan las columnas por las que se quiere ordenar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---------------+--------+----+--------+----------+--------+--------+--------+-----------+------------+\n",
      "|Rank|           Name|Platform|Year|   Genre| Publisher|NA_Sales|EU_Sales|JP_Sales|Other_Sales|Global_Sales|\n",
      "+----+---------------+--------+----+--------+----------+--------+--------+--------+-----------+------------+\n",
      "| 259|      Asteroids|    2600|1980| Shooter|     Atari|     4.0|    0.26|     0.0|       0.05|        4.31|\n",
      "| 545|Missile Command|    2600|1980| Shooter|     Atari|    2.56|    0.17|     0.0|       0.03|        2.76|\n",
      "|1768|        Kaboom!|    2600|1980|    Misc|Activision|    1.07|    0.07|     0.0|       0.01|        1.15|\n",
      "|1971|       Defender|    2600|1980|    Misc|     Atari|    0.99|    0.05|     0.0|       0.01|        1.05|\n",
      "|2671|         Boxing|    2600|1980|Fighting|Activision|    0.72|    0.04|     0.0|       0.01|        0.77|\n",
      "|4027|     Ice Hockey|    2600|1980|  Sports|Activision|    0.46|    0.03|     0.0|       0.01|        0.49|\n",
      "|5368|        Freeway|    2600|1980|  Action|Activision|    0.32|    0.02|     0.0|        0.0|        0.34|\n",
      "|6319|         Bridge|    2600|1980|    Misc|Activision|    0.25|    0.02|     0.0|        0.0|        0.27|\n",
      "|6898|       Checkers|    2600|1980|    Misc|     Atari|    0.22|    0.01|     0.0|        0.0|        0.24|\n",
      "|1108|    Ms. Pac-Man|    2600|1981|  Puzzle|     Atari|    1.54|     0.1|     0.0|       0.02|        1.65|\n",
      "+----+---------------+--------+----+--------+----------+--------+--------+--------+-----------+------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Order by\n",
    "df.orderBy(\"Year\", 'Platform').show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------+--------------------+------------------+\n",
      "|Platform|     Genre|   sum(Global_Sales)|     sum(NA_Sales)|\n",
      "+--------+----------+--------------------+------------------+\n",
      "|      PS|    Action|  127.05000039748847| 62.87999978289008|\n",
      "|     NES|    Puzzle|  20.999999906867743|  9.13999992236495|\n",
      "|    WiiU|  Strategy|  1.2400000132620335|0.5099999867379665|\n",
      "|     GBA|  Platform|   78.29999975115061| 45.81000040844083|\n",
      "|     Wii|  Fighting|  23.859999924898148|13.149999974295497|\n",
      "|    XOne|      Misc|   6.860000053420663| 4.439999930560589|\n",
      "|    XOne|  Fighting|  2.3100000210106373| 1.580000001937151|\n",
      "|     3DO|Simulation|0.019999999552965164|               0.0|\n",
      "|     PS4|    Action|   87.05999964103103|29.699999878183007|\n",
      "|      PS|    Puzzle|  12.080000067129731| 5.289999892935157|\n",
      "|      GB|    Puzzle|  47.470000356435776|29.350000709295273|\n",
      "|    X360|  Strategy|  10.130000134930015| 6.489999949932098|\n",
      "|      GC|    Racing|   21.88999987952411|14.929999867454171|\n",
      "|    XOne|    Action|  33.789999939501286|19.450000086799264|\n",
      "|     PS2|    Racing|  156.27999876625836| 75.71999965049326|\n",
      "|     PS2|  Platform|   72.50999970547855| 37.59999969229102|\n",
      "|      GC|    Puzzle|   4.699999975040555| 2.949999997392297|\n",
      "|      GC|    Sports|  25.490000020712614| 18.37000005505979|\n",
      "|    WiiU|    Puzzle|  1.3300000559538603|0.6600000038743019|\n",
      "|     GBA|Simulation|   5.910000009462237|3.6100000124424696|\n",
      "+--------+----------+--------------------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Agrupación de resultados\n",
    "df.groupBy('Platform', \"Genre\").sum('Global_Sales', 'NA_Sales').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Uso de SQL y tablas temporales dentro de PySpark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+--------------------+----+--------------------+--------+------------+\n",
      "|       Genre|                Name|Rank|           Publisher|NA_Sales|Global_Sales|\n",
      "+------------+--------------------+----+--------------------+--------+------------+\n",
      "|      Sports|          Wii Sports|   1|            Nintendo|   41.49|       82.74|\n",
      "|    Platform|   Super Mario Bros.|   2|            Nintendo|   29.08|       40.24|\n",
      "|      Racing|      Mario Kart Wii|   3|            Nintendo|   15.85|       35.82|\n",
      "|      Sports|   Wii Sports Resort|   4|            Nintendo|   15.75|        33.0|\n",
      "|Role-Playing|Pokemon Red/Pokem...|   5|            Nintendo|   11.27|       31.37|\n",
      "|      Puzzle|              Tetris|   6|            Nintendo|    23.2|       30.26|\n",
      "|    Platform|New Super Mario B...|   7|            Nintendo|   11.38|       30.01|\n",
      "|        Misc|            Wii Play|   8|            Nintendo|   14.03|       29.02|\n",
      "|    Platform|New Super Mario B...|   9|            Nintendo|   14.59|       28.62|\n",
      "|     Shooter|           Duck Hunt|  10|            Nintendo|   26.93|       28.31|\n",
      "|  Simulation|          Nintendogs|  11|            Nintendo|    9.07|       24.76|\n",
      "|      Racing|       Mario Kart DS|  12|            Nintendo|    9.81|       23.42|\n",
      "|Role-Playing|Pokemon Gold/Poke...|  13|            Nintendo|     9.0|        23.1|\n",
      "|      Sports|             Wii Fit|  14|            Nintendo|    8.94|       22.72|\n",
      "|      Sports|        Wii Fit Plus|  15|            Nintendo|    9.09|        22.0|\n",
      "|        Misc|  Kinect Adventures!|  16|Microsoft Game St...|   14.97|       21.82|\n",
      "|      Action|  Grand Theft Auto V|  17|Take-Two Interactive|    7.01|        21.4|\n",
      "|      Action|Grand Theft Auto:...|  18|Take-Two Interactive|    9.43|       20.81|\n",
      "|    Platform|   Super Mario World|  19|            Nintendo|   12.78|       20.61|\n",
      "|        Misc|Brain Age: Train ...|  20|            Nintendo|    4.75|       20.22|\n",
      "+------------+--------------------+----+--------------------+--------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Crear tabla temporal\n",
    "df.createOrReplaceTempView('VGSALES')\n",
    "\n",
    "# Comando SQL\n",
    "sql_str = 'select Genre, Name, Rank, Publisher, NA_Sales, Global_Sales from VGSALES limit 20'\n",
    "\n",
    "# Ejecutar SQL\n",
    "spark.sql(sql_str).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------------------+---------------------------+---------------------------+\n",
      "| Genre|           Publisher|round(sum(Global_Sales), 2)|round(sum(Global_Sales), 2)|\n",
      "+------+--------------------+---------------------------+---------------------------+\n",
      "|Action|           mixi, Inc|                       0.86|                       0.86|\n",
      "|Action|     dramatic create|                       0.01|                       0.01|\n",
      "|Action|         Zushi Games|                       0.02|                       0.02|\n",
      "|Action|           Zoo Games|                       0.27|                       0.27|\n",
      "|Action|Zoo Digital Publi...|                       0.78|                       0.78|\n",
      "|Action|                Yeti|                       0.01|                       0.01|\n",
      "|Action|         Xseed Games|                       0.11|                       0.11|\n",
      "|Action|  Wizard Video Games|                       0.62|                       0.62|\n",
      "|Action|Warner Bros. Inte...|                     118.24|                     118.24|\n",
      "|Action|             Wanadoo|                        0.6|                        0.6|\n",
      "|Action|       Vivendi Games|                       12.6|                       12.6|\n",
      "|Action|  Virgin Interactive|                      20.35|                      20.35|\n",
      "|Action|       Vir2L Studios|                       0.16|                       0.16|\n",
      "|Action|               Views|                       0.02|                       0.02|\n",
      "|Action|        Video System|                       0.19|                       0.19|\n",
      "|Action|             Unknown|                       4.32|                       4.32|\n",
      "|Action|Universal Interac...|                       3.68|                       3.68|\n",
      "|Action|     Universal Gamex|                       0.63|                       0.63|\n",
      "|Action|      Ubisoft Annecy|                       3.38|                       3.38|\n",
      "|Action|             Ubisoft|                     142.94|                     142.94|\n",
      "+------+--------------------+---------------------------+---------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.createOrReplaceGlobalTempView('VGSALES')\n",
    "sql_str =  \"\"\"\n",
    "                select  Genre, \n",
    "                        Publisher,\n",
    "                        round(sum(Global_Sales),2),\n",
    "                        round(sum(Global_Sales),2) \n",
    "                from VGSALES \n",
    "                group by Genre, Publisher \n",
    "                order by Genre, Publisher desc\n",
    "\"\"\"\n",
    "\n",
    "spark.sql(sql_str).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Big Data Parte 2  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------------------------------------+---------------------------+---------------------------+\n",
      "| Genre|                             Publisher|round(sum(Global_Sales), 2)|round(sum(Global_Sales), 2)|\n",
      "+------+--------------------------------------+---------------------------+---------------------------+\n",
      "|Action|                             mixi, Inc|                       0.86|                       0.86|\n",
      "|Action|                       dramatic create|                       0.01|                       0.01|\n",
      "|Action|                           Zushi Games|                       0.02|                       0.02|\n",
      "|Action|                             Zoo Games|                       0.27|                       0.27|\n",
      "|Action|                Zoo Digital Publishing|                       0.78|                       0.78|\n",
      "|Action|                                  Yeti|                       0.01|                       0.01|\n",
      "|Action|                           Xseed Games|                       0.11|                       0.11|\n",
      "|Action|                    Wizard Video Games|                       0.62|                       0.62|\n",
      "|Action|Warner Bros. Interactive Entertainment|                     118.24|                     118.24|\n",
      "|Action|                               Wanadoo|                        0.6|                        0.6|\n",
      "+------+--------------------------------------+---------------------------+---------------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.createOrReplaceGlobalTempView('VGSALES')\n",
    "sql_str =  \"\"\"\n",
    "                select  Genre, \n",
    "                        Publisher,\n",
    "                        round(sum(Global_Sales),2),\n",
    "                        round(sum(Global_Sales),2) \n",
    "                from VGSALES \n",
    "                group by Genre, Publisher \n",
    "                order by Genre, Publisher desc\n",
    "\"\"\"\n",
    "\n",
    "# El 10 limita  mis resultados a 10 líneas, el 80 hace que el máximo de caracteres que se muestran en una celda se extienda hasta 80\n",
    "spark.sql(sql_str).show(10, 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-RECORD 0---------------------------------------------\n",
      " Genre                       | Action                 \n",
      " Publisher                   | mixi, Inc              \n",
      " round(sum(Global_Sales), 2) | 0.86                   \n",
      " round(sum(Global_Sales), 2) | 0.86                   \n",
      "-RECORD 1---------------------------------------------\n",
      " Genre                       | Action                 \n",
      " Publisher                   | dramatic create        \n",
      " round(sum(Global_Sales), 2) | 0.01                   \n",
      " round(sum(Global_Sales), 2) | 0.01                   \n",
      "-RECORD 2---------------------------------------------\n",
      " Genre                       | Action                 \n",
      " Publisher                   | Zushi Games            \n",
      " round(sum(Global_Sales), 2) | 0.02                   \n",
      " round(sum(Global_Sales), 2) | 0.02                   \n",
      "-RECORD 3---------------------------------------------\n",
      " Genre                       | Action                 \n",
      " Publisher                   | Zoo Games              \n",
      " round(sum(Global_Sales), 2) | 0.27                   \n",
      " round(sum(Global_Sales), 2) | 0.27                   \n",
      "-RECORD 4---------------------------------------------\n",
      " Genre                       | Action                 \n",
      " Publisher                   | Zoo Digital Publishing \n",
      " round(sum(Global_Sales), 2) | 0.78                   \n",
      " round(sum(Global_Sales), 2) | 0.78                   \n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.createOrReplaceGlobalTempView('VGSALES')\n",
    "sql_str =  \"\"\"\n",
    "                select  Genre, \n",
    "                        Publisher,\n",
    "                        round(sum(Global_Sales),2),\n",
    "                        round(sum(Global_Sales),2) \n",
    "                from VGSALES \n",
    "                group by Genre, Publisher \n",
    "                order by Genre, Publisher desc\n",
    "\"\"\"\n",
    "\n",
    "# El True hace que el display sea en forma de tarjeta\n",
    "spark.sql(sql_str).show(5, 80, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Particionamiento de Datos - partitionBy()\n",
    "Debido a la cantidad de información que podemos estar manejando en Spark, podemos dividir la data en pedazos más pequeños. La partición de datos consiste en dividir esa información en porciones más pequeñas pbasadas en algún criterio. Esto se usa para mejorar el perfomrmance en general. Para poder paralelizar el procesamiento.\n",
    "\n",
    "### Tres Tips:\n",
    "- No usar tamaños extremos - No particionar en datasets ni muy grandes ni muy pequeñas. Tratar de hacer que todas las particiones tengan aproximadamente el mismo tamaño. Se recomiendan tamaños de 256Mb a 1Gb por particion\n",
    "- No usar IDs para hacer la partición puesto que esto resultaría en muchas particiones de un registro\n",
    "- Usar columnas para particionar la data que van a ser usadas en sentencias groupBy\n",
    "\n",
    "### Tres tipos de particiones:\n",
    "- PartitionBy - Va a cambiar la estructura del folder uysando el criterio que se le especifique. \"Cortame por Género o por país o por plataforma\".  No controla cuántos archivos se generan debajo de cada folder. se puede combinar repartition + partitionBy\n",
    "- Repartition - Este método puede incrementar o reducir el # de particiones. Aqui se especifíca el número de particiones y los registros que hay en cada una se distribuyen al azar. \n",
    "- Coalesce - Reduce el número de particiones en un DataFrame. En vez de crear nuevas particiones mueve la data y la ajusta a las particiones existentes, por lo que solamente puede reducir el número de particiones. Es más barato computacionalmente.\n",
    "\n",
    "En términos prácticos se verá la creación de muchos archivos en los que cada método genera la data. Estos podrán ser varios volders, varios archiovs o una combinación de ambos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Rank: string (nullable = true)\n",
      " |-- Name: string (nullable = true)\n",
      " |-- Platform: string (nullable = true)\n",
      " |-- Year: string (nullable = true)\n",
      " |-- Genre: string (nullable = true)\n",
      " |-- Publisher: string (nullable = true)\n",
      " |-- NA_Sales: float (nullable = true)\n",
      " |-- EU_Sales: float (nullable = true)\n",
      " |-- JP_Sales: float (nullable = true)\n",
      " |-- Other_Sales: float (nullable = true)\n",
      " |-- Global_Sales: float (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Rank: string (nullable = true)\n",
      " |-- Name: string (nullable = true)\n",
      " |-- Platform: string (nullable = true)\n",
      " |-- Year: string (nullable = true)\n",
      " |-- Genre: string (nullable = true)\n",
      " |-- Publisher: string (nullable = true)\n",
      " |-- NA_Sales: float (nullable = true)\n",
      " |-- EU_Sales: float (nullable = true)\n",
      " |-- JP_Sales: float (nullable = true)\n",
      " |-- Other_Sales: float (nullable = true)\n",
      " |-- Global_Sales: float (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# path_archivo = \"D:/DataAnalysis_EBAC/ebac/Python/Modulo23/supermarket_sales.csv\"\n",
    "\n",
    "# df = spark.read.csv(path_archivo, header=True)\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o178.save.\n: java.lang.UnsatisfiedLinkError: 'boolean org.apache.hadoop.io.nativeio.NativeIO$Windows.access0(java.lang.String, int)'\r\n\tat org.apache.hadoop.io.nativeio.NativeIO$Windows.access0(Native Method)\r\n\tat org.apache.hadoop.io.nativeio.NativeIO$Windows.access(NativeIO.java:793)\r\n\tat org.apache.hadoop.fs.FileUtil.canRead(FileUtil.java:1249)\r\n\tat org.apache.hadoop.fs.FileUtil.list(FileUtil.java:1454)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.listStatus(RawLocalFileSystem.java:601)\r\n\tat org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:1972)\r\n\tat org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:2014)\r\n\tat org.apache.hadoop.fs.ChecksumFileSystem.listStatus(ChecksumFileSystem.java:761)\r\n\tat org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:1972)\r\n\tat org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:2014)\r\n\tat org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.getAllCommittedTaskPaths(FileOutputCommitter.java:334)\r\n\tat org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.commitJobInternal(FileOutputCommitter.java:404)\r\n\tat org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.commitJob(FileOutputCommitter.java:377)\r\n\tat org.apache.spark.internal.io.HadoopMapReduceCommitProtocol.commitJob(HadoopMapReduceCommitProtocol.scala:192)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$writeAndCommit$3(FileFormatWriter.scala:275)\r\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\r\n\tat org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:552)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.writeAndCommit(FileFormatWriter.scala:275)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeWrite(FileFormatWriter.scala:304)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:190)\r\n\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:190)\r\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:113)\r\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:111)\r\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:125)\r\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:107)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)\r\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:107)\r\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:461)\r\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:461)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:437)\r\n\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:98)\r\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:85)\r\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:83)\r\n\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:142)\r\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:859)\r\n\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:388)\r\n\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:361)\r\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:240)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:75)\r\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:52)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:580)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.base/java.lang.Thread.run(Thread.java:1583)\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[25], line 8\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# import os\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m# import sys\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m      6\u001b[0m \n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# df.write.option('header', True).partitionBy('Platform').mode('overwrite').csv('/partition/platform')\u001b[39;00m\n\u001b[1;32m----> 8\u001b[0m \u001b[43mdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moption\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mheader\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpartitionBy\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mPlatform\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43moverwrite\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mformat\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcsv\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mpartitions\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\Spark\\spark-3.5.1-bin-hadoop3\\python\\pyspark\\sql\\readwriter.py:1463\u001b[0m, in \u001b[0;36mDataFrameWriter.save\u001b[1;34m(self, path, format, mode, partitionBy, **options)\u001b[0m\n\u001b[0;32m   1461\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jwrite\u001b[38;5;241m.\u001b[39msave()\n\u001b[0;32m   1462\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1463\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jwrite\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\Spark\\spark-3.5.1-bin-hadoop3\\python\\lib\\py4j-0.10.9.7-src.zip\\py4j\\java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[1;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[0;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[1;32mC:\\Spark\\spark-3.5.1-bin-hadoop3\\python\\pyspark\\errors\\exceptions\\captured.py:179\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    177\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m    178\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 179\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    180\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    181\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[1;32mC:\\Spark\\spark-3.5.1-bin-hadoop3\\python\\lib\\py4j-0.10.9.7-src.zip\\py4j\\protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[0;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[1;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[0;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[0;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[0;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[0;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[0;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling o178.save.\n: java.lang.UnsatisfiedLinkError: 'boolean org.apache.hadoop.io.nativeio.NativeIO$Windows.access0(java.lang.String, int)'\r\n\tat org.apache.hadoop.io.nativeio.NativeIO$Windows.access0(Native Method)\r\n\tat org.apache.hadoop.io.nativeio.NativeIO$Windows.access(NativeIO.java:793)\r\n\tat org.apache.hadoop.fs.FileUtil.canRead(FileUtil.java:1249)\r\n\tat org.apache.hadoop.fs.FileUtil.list(FileUtil.java:1454)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.listStatus(RawLocalFileSystem.java:601)\r\n\tat org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:1972)\r\n\tat org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:2014)\r\n\tat org.apache.hadoop.fs.ChecksumFileSystem.listStatus(ChecksumFileSystem.java:761)\r\n\tat org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:1972)\r\n\tat org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:2014)\r\n\tat org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.getAllCommittedTaskPaths(FileOutputCommitter.java:334)\r\n\tat org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.commitJobInternal(FileOutputCommitter.java:404)\r\n\tat org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.commitJob(FileOutputCommitter.java:377)\r\n\tat org.apache.spark.internal.io.HadoopMapReduceCommitProtocol.commitJob(HadoopMapReduceCommitProtocol.scala:192)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$writeAndCommit$3(FileFormatWriter.scala:275)\r\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\r\n\tat org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:552)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.writeAndCommit(FileFormatWriter.scala:275)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeWrite(FileFormatWriter.scala:304)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:190)\r\n\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:190)\r\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:113)\r\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:111)\r\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:125)\r\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:107)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)\r\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:107)\r\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:461)\r\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:461)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:437)\r\n\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:98)\r\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:85)\r\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:83)\r\n\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:142)\r\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:859)\r\n\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:388)\r\n\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:361)\r\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:240)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:75)\r\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:52)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:580)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.base/java.lang.Thread.run(Thread.java:1583)\r\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# import os\n",
    "# import sys\n",
    "\n",
    "# os.environ['PYSPARK_PYTHON'] = sys.executable\n",
    "# os.environ['PYSPARK_DRIVER_PYTHON'] = sys.executable\n",
    "\n",
    "# df.write.option('header', True).partitionBy('Platform').mode('overwrite').csv('/partition/platform')\n",
    "df.write.option('header', True).partitionBy('Platform').mode('overwrite').format('csv').save('partitions')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o559.csv.\n: java.lang.UnsatisfiedLinkError: 'boolean org.apache.hadoop.io.nativeio.NativeIO$Windows.access0(java.lang.String, int)'\r\n\tat org.apache.hadoop.io.nativeio.NativeIO$Windows.access0(Native Method)\r\n\tat org.apache.hadoop.io.nativeio.NativeIO$Windows.access(NativeIO.java:793)\r\n\tat org.apache.hadoop.fs.FileUtil.canRead(FileUtil.java:1249)\r\n\tat org.apache.hadoop.fs.FileUtil.list(FileUtil.java:1454)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.listStatus(RawLocalFileSystem.java:601)\r\n\tat org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:1972)\r\n\tat org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:2014)\r\n\tat org.apache.hadoop.fs.ChecksumFileSystem.listStatus(ChecksumFileSystem.java:761)\r\n\tat org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:1972)\r\n\tat org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:2014)\r\n\tat org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.getAllCommittedTaskPaths(FileOutputCommitter.java:334)\r\n\tat org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.commitJobInternal(FileOutputCommitter.java:404)\r\n\tat org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.commitJob(FileOutputCommitter.java:377)\r\n\tat org.apache.spark.internal.io.HadoopMapReduceCommitProtocol.commitJob(HadoopMapReduceCommitProtocol.scala:192)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$writeAndCommit$3(FileFormatWriter.scala:275)\r\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\r\n\tat org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:552)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.writeAndCommit(FileFormatWriter.scala:275)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeWrite(FileFormatWriter.scala:304)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:190)\r\n\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:190)\r\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:113)\r\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:111)\r\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:125)\r\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:107)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)\r\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:107)\r\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:461)\r\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:461)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:437)\r\n\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:98)\r\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:85)\r\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:83)\r\n\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:142)\r\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:859)\r\n\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:388)\r\n\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:361)\r\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:240)\r\n\tat org.apache.spark.sql.DataFrameWriter.csv(DataFrameWriter.scala:850)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:75)\r\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:52)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:580)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.base/java.lang.Thread.run(Thread.java:1583)\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[82], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcsv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdata.csv\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\Spark\\spark-3.5.1-bin-hadoop3\\python\\pyspark\\sql\\readwriter.py:1864\u001b[0m, in \u001b[0;36mDataFrameWriter.csv\u001b[1;34m(self, path, mode, compression, sep, quote, escape, header, nullValue, escapeQuotes, quoteAll, dateFormat, timestampFormat, ignoreLeadingWhiteSpace, ignoreTrailingWhiteSpace, charToEscapeQuoteEscaping, encoding, emptyValue, lineSep)\u001b[0m\n\u001b[0;32m   1845\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmode(mode)\n\u001b[0;32m   1846\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_set_opts(\n\u001b[0;32m   1847\u001b[0m     compression\u001b[38;5;241m=\u001b[39mcompression,\n\u001b[0;32m   1848\u001b[0m     sep\u001b[38;5;241m=\u001b[39msep,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1862\u001b[0m     lineSep\u001b[38;5;241m=\u001b[39mlineSep,\n\u001b[0;32m   1863\u001b[0m )\n\u001b[1;32m-> 1864\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jwrite\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcsv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\Spark\\spark-3.5.1-bin-hadoop3\\python\\lib\\py4j-0.10.9.7-src.zip\\py4j\\java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[1;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[0;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[1;32mC:\\Spark\\spark-3.5.1-bin-hadoop3\\python\\pyspark\\errors\\exceptions\\captured.py:179\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    177\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m    178\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 179\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    180\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    181\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[1;32mC:\\Spark\\spark-3.5.1-bin-hadoop3\\python\\lib\\py4j-0.10.9.7-src.zip\\py4j\\protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[0;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[1;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[0;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[0;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[0;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[0;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[0;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling o559.csv.\n: java.lang.UnsatisfiedLinkError: 'boolean org.apache.hadoop.io.nativeio.NativeIO$Windows.access0(java.lang.String, int)'\r\n\tat org.apache.hadoop.io.nativeio.NativeIO$Windows.access0(Native Method)\r\n\tat org.apache.hadoop.io.nativeio.NativeIO$Windows.access(NativeIO.java:793)\r\n\tat org.apache.hadoop.fs.FileUtil.canRead(FileUtil.java:1249)\r\n\tat org.apache.hadoop.fs.FileUtil.list(FileUtil.java:1454)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.listStatus(RawLocalFileSystem.java:601)\r\n\tat org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:1972)\r\n\tat org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:2014)\r\n\tat org.apache.hadoop.fs.ChecksumFileSystem.listStatus(ChecksumFileSystem.java:761)\r\n\tat org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:1972)\r\n\tat org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:2014)\r\n\tat org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.getAllCommittedTaskPaths(FileOutputCommitter.java:334)\r\n\tat org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.commitJobInternal(FileOutputCommitter.java:404)\r\n\tat org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.commitJob(FileOutputCommitter.java:377)\r\n\tat org.apache.spark.internal.io.HadoopMapReduceCommitProtocol.commitJob(HadoopMapReduceCommitProtocol.scala:192)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$writeAndCommit$3(FileFormatWriter.scala:275)\r\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\r\n\tat org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:552)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.writeAndCommit(FileFormatWriter.scala:275)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeWrite(FileFormatWriter.scala:304)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:190)\r\n\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:190)\r\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:113)\r\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:111)\r\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:125)\r\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:107)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)\r\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:107)\r\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:461)\r\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:461)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:437)\r\n\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:98)\r\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:85)\r\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:83)\r\n\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:142)\r\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:859)\r\n\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:388)\r\n\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:361)\r\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:240)\r\n\tat org.apache.spark.sql.DataFrameWriter.csv(DataFrameWriter.scala:850)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:75)\r\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:52)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:580)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.base/java.lang.Thread.run(Thread.java:1583)\r\n"
     ]
    }
   ],
   "source": [
    "df.write.csv('data.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
